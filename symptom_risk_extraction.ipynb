{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b311b908",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9339101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327fe08",
   "metadata": {},
   "source": [
    "# Supporting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc64eb3",
   "metadata": {},
   "source": [
    "## load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f253bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    Load ./data/conditions_data_post_stitched.csv and return pd.DataFrame\n",
    "    '''\n",
    "    df = pd.read_csv('./data/BeyondBlue/conditions_data_post_stitched.csv')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2098f2",
   "metadata": {},
   "source": [
    "## sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9049051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(df, n=100):\n",
    "    '''\n",
    "    Randomly sample n rows from the DataFrame with fixed random state.\n",
    "    '''\n",
    "    return df.sample(n, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43410a",
   "metadata": {},
   "source": [
    "## clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3273ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"  \n",
    "    Cleans and standardizes text by removing leading/trailing whitespace,\n",
    "    replacing newlines, tabs, non-breaking spaces with a single space, multiple spaces,\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be formatted.\n",
    "    Returns:\n",
    "        str: The cleaned and standardized text.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5cd8b",
   "metadata": {},
   "source": [
    "## concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab859a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(row):\n",
    "    \"\"\"\n",
    "    Concatenates Post Title and Post Content with a period if the Post Title does not end with a punctuation mark.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row from the DataFrame containing 'Post_Title' and 'Post_Content'.\n",
    "    Returns:\n",
    "        str: Concatenated string of Post Title and Post Content.\n",
    "    \"\"\"\n",
    "    end_punctuations = {'!', '?'}\n",
    "    if row['Post_Title'] and row['Post_Title'][-1] in end_punctuations:\n",
    "        return row['Post_Title'] + ' ' + row['Post_Content']\n",
    "    else:\n",
    "        return row['Post_Title'].rstrip() + '. ' + row['Post_Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74db23",
   "metadata": {},
   "source": [
    "## clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11935348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    Clean the DataFrame by concatenating Post Title and Content, clean the text, and return only 'Post_ID' and 'Post_Title_Content' columns\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'Post_ID', 'Post_Title', and 'Post_Content' columns.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Post_ID' and 'Post_Title_Content' columns.\n",
    "    '''\n",
    "    df['Post_Title_Content'] = df.apply(concat, axis=1)\n",
    "    df['Post_Title_Content'] = df['Post_Title_Content'].apply(clean_text)\n",
    "    return df[['Post_ID', 'Post_Title_Content']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480eb159",
   "metadata": {},
   "source": [
    "## embed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a9a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data(\n",
    "        df, model_name='multi-qa-mpnet-base-dot-v1', \n",
    "        mode = \"3-gram\", output_csv='./data/sample/data_embeddings.csv',\n",
    "        load = True, save = False\n",
    "    ):\n",
    "    '''\n",
    "    Embeds the DataFrame using a model_name. \n",
    "    If load is True, it will load existing embeddings from the CSV, else it will compute new embeddings.\n",
    "    If save is True, it will save the embeddings to the CSV, else it will not save the embeddings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'Post_ID' and 'Post_Title_Content'.\n",
    "        model_name (str): Name of the SBERT model to use for embeddings.\n",
    "        mode (str): Mode for text splitting (e.g., \"3-gram\").\n",
    "        output_csv (str): Path to the output CSV file.\n",
    "        load (bool): Whether to load existing embeddings from the CSV.\n",
    "        save (bool): Whether to save new embeddings to the CSV.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Post_ID' and 'Embedding' columns.\n",
    "    '''\n",
    "    \n",
    "    # Check if embeddings already exist\n",
    "    if load and os.path.exists(output_csv):\n",
    "        df = pd.read_csv(output_csv, index_col=0)\n",
    "        meta_df = df.iloc[:,:3]\n",
    "        emb_df = df.iloc[:, 3:]\n",
    "        return meta_df, emb_df\n",
    "    \n",
    "    # Load SBERT model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    all_records = []\n",
    "    \n",
    "    for _ , row in tqdm(df.iterrows(), total=len(df), desc=\"Embedding data\"):\n",
    "\n",
    "        # sentence tokenize text\n",
    "        sentence_list = sent_tokenize(row['Post_Title_Content'])\n",
    "\n",
    "        # Create n-grams based on mode\n",
    "        if mode == '3-gram':\n",
    "            # Create n-grams from sentences\n",
    "            n_grams_list = []\n",
    "            clause_list = []\n",
    "            # Tokenize and create n-grams for each sentence\n",
    "            for sentence in sentence_list:\n",
    "                # Strip whitespace\n",
    "                sentence = sentence.strip()\n",
    "                # Split sentence into raw clauses (based on commas)\n",
    "                if \",\" in sentence:\n",
    "                    # Create 3-grams from clauses\n",
    "                    clauses = sentence.split(\",\")\n",
    "                    # Tokenize and create n-grams for each clause\n",
    "                    for clause in clauses:\n",
    "                        # strip whitespace\n",
    "                        clause = clause.strip()\n",
    "                        # Word tokenize clause\n",
    "                        tokens = word_tokenize(clause)\n",
    "                        # Remove punctuation\n",
    "                        tokens = [token.strip() for token in tokens if token not in string.punctuation]\n",
    "                        # If clause is 1 or 2-gram\n",
    "                        if len(tokens) < 3:\n",
    "                            # append as is\n",
    "                            n_grams_list.append(' '.join(tokens))\n",
    "                            clause_list.append(clause)\n",
    "                        # else if clause is 3-gram\n",
    "                        elif len(tokens) == 3:\n",
    "                            # create 2-grams\n",
    "                            clause_bigrams = list(ngrams(tokens, 2))\n",
    "                            # join the words in the bigrams\n",
    "                            clause_bigrams = [' '.join(bigram) for bigram in clause_bigrams]\n",
    "                            # append the bigrams and the original clause to the n-grams list\n",
    "                            n_grams_list += [clause_bigrams[0], ' '.join(tokens), clause_bigrams[1]]\n",
    "                            # append the original clause to the clause list equal to the number of n-grams\n",
    "                            clause_list += [clause]*(len(clause_bigrams) + 1)\n",
    "                        # else create n-grams but also with 2-gram for the start and end\n",
    "                        else:                            \n",
    "                            # create 2-grams\n",
    "                            clause_bigrams = list(ngrams(tokens, 2))\n",
    "                            # join the words in the bigrams\n",
    "                            clause_bigrams = [' '.join(bigram) for bigram in clause_bigrams]\n",
    "                            # create 3-grams\n",
    "                            clause_ngrams = list(ngrams(tokens, 3))\n",
    "                            # join the words in the 3-grams\n",
    "                            clause_ngrams = [' '.join(trigram) for trigram in clause_ngrams]\n",
    "                            # append the first bigram to the n-grams list\n",
    "                            n_grams_list.append(clause_bigrams[0])\n",
    "                            # append the trigrams to the n-grams list\n",
    "                            n_grams_list += clause_ngrams\n",
    "                            # append the last bigram to the n-grams list\n",
    "                            n_grams_list.append(clause_bigrams[-1])\n",
    "                            # append the original clause to the clause list equal to the number of n-grams\n",
    "                            clause_list += [clause]*(len(clause_ngrams) + 2)\n",
    "                else:\n",
    "                    # If no commas, treat the whole sentence as a single clause\n",
    "                    # Word tokenize sentence\n",
    "                    tokens = word_tokenize(sentence)\n",
    "                    tokens = [token.strip() for token in tokens if token not in string.punctuation]\n",
    "                    # If clause is 1 or 2-gram\n",
    "                    if len(tokens) < 3:\n",
    "                        # append as is\n",
    "                        n_grams_list.append(' '.join(tokens))\n",
    "                        clause_list.append(sentence)\n",
    "                    # else if clause is 3-gram\n",
    "                    elif len(tokens) == 3:\n",
    "                        # create 2-grams\n",
    "                        clause_bigrams = list(ngrams(tokens, 2))\n",
    "                        # join the words in the bigrams\n",
    "                        clause_bigrams = [' '.join(bigram) for bigram in clause_bigrams]\n",
    "                        # append the bigrams and the original clause to the n-grams list\n",
    "                        n_grams_list += [clause_bigrams[0], ' '.join(tokens), clause_bigrams[1]]\n",
    "                        # append the original clause to the clause list equal to the number of n-grams\n",
    "                        clause_list += [sentence]*(len(clause_bigrams) + 1)\n",
    "                    # else create n-grams but also with 2-gram for the start and end\n",
    "                    else:                            \n",
    "                        # create 2-grams\n",
    "                        clause_bigrams = list(ngrams(tokens, 2))\n",
    "                        # join the words in the bigrams\n",
    "                        clause_bigrams = [' '.join(bigram) for bigram in clause_bigrams]\n",
    "                        # create 3-grams\n",
    "                        clause_ngrams = list(ngrams(tokens, 3))\n",
    "                        # join the words in the 3-grams\n",
    "                        clause_ngrams = [' '.join(trigram) for trigram in clause_ngrams]\n",
    "                        # append the first bigram to the n-grams list\n",
    "                        n_grams_list.append(clause_bigrams[0])\n",
    "                        # append the trigrams to the n-grams list\n",
    "                        n_grams_list += clause_ngrams\n",
    "                        # append the last bigram to the n-grams list\n",
    "                        n_grams_list.append(clause_bigrams[-1])\n",
    "                        # append the original clause to the clause list equal to the number of n-grams\n",
    "                        clause_list += [sentence]*(len(clause_ngrams) + 2)\n",
    "            # Create a copy of the n-grams list\n",
    "            n_grams_list_copy = n_grams_list.copy()\n",
    "            # Create a copy of the clause list\n",
    "            clause_list_copy = clause_list.copy()\n",
    "\n",
    "        # SBERT embeddings\n",
    "        embeddings = model.encode(n_grams_list_copy, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "        # Collect data\n",
    "        for text_idx, (clause, n_gram, embedding) in enumerate(zip(clause_list_copy, n_grams_list_copy, embeddings)):\n",
    "            all_records.append({\n",
    "                'Post_ID': row['Post_ID'],\n",
    "                'clause': clause,\n",
    "                'n_gram': n_gram,\n",
    "                'embedding': embedding.tolist()  # store as list for CSV-friendly format\n",
    "            })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Split embedding into separate columns and create embedding DataFrame\n",
    "    emb_dim = len(df['embedding'][0])\n",
    "    emb_cols = [f'emb_{i}' for i in range(emb_dim)]\n",
    "    emb_df = pd.DataFrame(df['embedding'].tolist(), columns=emb_cols)\n",
    "\n",
    "    # Meta data DataFrame\n",
    "    meta_df = df[['Post_ID', 'clause', 'n_gram']]\n",
    "\n",
    "    if save:\n",
    "        # Save embeddings to CSV\n",
    "        final_df = pd.concat([meta_df, emb_df], axis=1)\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "    # Export to CSV\n",
    "    final_df.to_csv(output_csv, index=True)\n",
    "\n",
    "    return meta_df, emb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336df7de",
   "metadata": {},
   "source": [
    "## load_symptom_repr_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e87114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_symptom_repr_words():\n",
    "    '''\n",
    "    Loads symptom representative words from ./data/symptom_list_v3.csv.\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        A list of symptom representative words.\n",
    "    '''\n",
    "    df = pd.read_csv('./data/symptoms_list_v3.csv', sep=';')\n",
    "    repr_words = df['representative_words']\n",
    "    # Turn each comma‑separated string into a list of trimmed items\n",
    "    repr_words = repr_words.apply(\n",
    "        lambda x: [p.strip() for p in x.split(',')]\n",
    "    )\n",
    "    # Flatten the list of lists\n",
    "    repr_words = [item for sublist in repr_words for item in sublist]\n",
    "    # Remove duplicates\n",
    "    repr_words = list(set(repr_words))\n",
    "    return repr_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba8910",
   "metadata": {},
   "source": [
    "## load_risk_factor_repr_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5892cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_risk_factor_repr_words():\n",
    "    '''\n",
    "    Loads risk representative words from ./data/risk_factors.csv.\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        A list of risk representative words.\n",
    "    '''\n",
    "    df = pd.read_csv('./data/risk_factors.csv', sep=',')\n",
    "    repr_words = df['representative_words']\n",
    "    # Turn each comma‑separated string into a list of trimmed items\n",
    "    repr_words = repr_words.apply(\n",
    "        lambda x: [p.strip() for p in x.split(';')]\n",
    "    )\n",
    "    # Flatten the list of lists\n",
    "    repr_words = [item for sublist in repr_words for item in sublist]\n",
    "    # Remove duplicates\n",
    "    repr_words = list(set(repr_words))\n",
    "    return repr_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe085b9",
   "metadata": {},
   "source": [
    "## embed_rep_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b51a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_rep_words(\n",
    "       symptoms_rw_list, risk_factors_rw_list,\n",
    "       model_name='multi-qa-mpnet-base-dot-v1',\n",
    "       output_csv='./data/sample/representative_word_embeddings.csv',\n",
    "       load = True, save = False\n",
    "):\n",
    "    '''\n",
    "    Embeds risk factor and symptom representative words from the DataFrame.\n",
    "    Args:\n",
    "        symptoms_wr_list: List of symptom representative words.\n",
    "        risk_factors_wr_list: List of risk factor representative words.\n",
    "        model_name: Name of the SBERT model to use.\n",
    "        output_csv: Path to the output CSV file.\n",
    "        load: Whether to load existing embeddings from the CSV file.\n",
    "        save: Whether to save new embeddings to the CSV file.\n",
    "    '''\n",
    "\n",
    "    # Check if load is True and embeddings already exist\n",
    "    if load and os.path.exists(output_csv):\n",
    "        df = pd.read_csv(output_csv)\n",
    "        meta_df = df.iloc[:,:1]\n",
    "        emb_df = df.iloc[:, 1:]\n",
    "        return meta_df, emb_df\n",
    "    \n",
    "    # Combine representative words\n",
    "    repr_words = symptoms_rw_list + risk_factors_rw_list\n",
    "\n",
    "    # Load SBERT model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Initialize list to hold all records\n",
    "    all_records = []\n",
    "\n",
    "    # Get embeddings\n",
    "    embeddings = model.encode(repr_words, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # Collect data\n",
    "    for word, embedding in zip(repr_words, embeddings):\n",
    "        all_records.append({\n",
    "            'repr_word': word,\n",
    "            'embedding': embedding.tolist()  # store as list for CSV-friendly format\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Split embedding into separate columns and create embedding DataFrame\n",
    "    emb_dim = len(df['embedding'][0])\n",
    "    emb_cols = [f'emb_{i}' for i in range(emb_dim)]\n",
    "    emb_df = pd.DataFrame(df['embedding'].tolist(), columns=emb_cols)\n",
    "\n",
    "    # Meta data DataFrame\n",
    "    meta_df = df[['repr_word']]\n",
    "\n",
    "    if save:\n",
    "        # Save embeddings to CSV\n",
    "        final_df = pd.concat([meta_df, emb_df], axis=1)\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    return meta_df, emb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b29cb",
   "metadata": {},
   "source": [
    "## detect_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_variables(\n",
    "        data_meta_df, data_emb_df,\n",
    "        repr_words_meta_df, repr_words_emb_df,\n",
    "        threshold = 0.75,\n",
    "        load = True, save = False\n",
    "    ):\n",
    "    '''\n",
    "    Detects variables (representative words) in the posts by comparing embeddings using cosine similiarity.\n",
    "    Args:\n",
    "        data_meta_df: Metadata DataFrame for the posts.\n",
    "        data_emb_df: Embeddings DataFrame for the posts.\n",
    "        repr_words_meta_df: Metadata DataFrame for the representative words.\n",
    "        repr_words_emb_df: Embeddings DataFrame for the representative words.\n",
    "        threshold: Cosine similarity threshold for considering a match.\n",
    "        load: Whether to load existing detected variables from the CSV file.\n",
    "        save: Whether to save the detected variables to a CSV file.\n",
    "    Returns:\n",
    "        DataFrame containing the detected variables for each post.\n",
    "        DataFrame containing the detected variables for each ngrams.\n",
    "    '''\n",
    "    # define output CSV path\n",
    "    symptom_per_post_output_csv = './data/sample/detected_symptoms_per_post.csv'\n",
    "    symptom_per_ngram_output_csv = './data/sample/detected_symptoms_per_ngram.csv'\n",
    "    repr_words_per_post_output_csv = './data/sample/detected_repr_words_per_post.csv'\n",
    "    repr_words_per_ngram_output_csv = './data/sample/detected_repr_words_per_ngram.csv'\n",
    "\n",
    "    # Check if load is True and embeddings already exist\n",
    "    if load and os.path.exists(symptom_per_post_output_csv) and os.path.exists(symptom_per_ngram_output_csv) and os.path.exists(repr_words_per_post_output_csv) and os.path.exists(repr_words_per_ngram_output_csv):\n",
    "        symptoms_post_df = pd.read_csv(symptom_per_post_output_csv)\n",
    "        symptoms_ngram_df = pd.read_csv(symptom_per_ngram_output_csv)\n",
    "        repr_words_post_df = pd.read_csv(repr_words_per_post_output_csv)\n",
    "        repr_words_ngram_df = pd.read_csv(repr_words_per_ngram_output_csv)\n",
    "        return symptoms_post_df, symptoms_ngram_df, repr_words_post_df, repr_words_ngram_df\n",
    "\n",
    "    # Get representative words, post IDs, clauses, and ngrams\n",
    "    repr_words = repr_words_meta_df['repr_word'].tolist()\n",
    "    post_ids_list = data_meta_df['Post_ID'].tolist()\n",
    "    clause_list = data_meta_df['clause'].tolist()\n",
    "    ngram_list = data_meta_df['n_gram'].tolist()\n",
    "\n",
    "    # Initialize lists to store variable data\n",
    "    detected_variables_per_post = []\n",
    "    detected_variables_per_ngram = []\n",
    "\n",
    "    # Initialize post row\n",
    "    post_row = {'Post_ID': '', **{word:0 for word in repr_words}}\n",
    "\n",
    "    # Iterate through each ngram and its embedding (converted to numpy)\n",
    "    # while using tqdm for progress tracking\n",
    "    for post_id, clause, ngram, ngram_emb in tqdm(zip(post_ids_list, clause_list, ngram_list, data_emb_df.values), total=len(post_ids_list), desc=\"Detecting variables\"):\n",
    "        # Initialize variable for the current post\n",
    "        ngram_row = {'Post_ID': post_id, 'clause': clause, 'n_gram': ngram, **{word:0 for word in repr_words}}\n",
    "        # Initialise controller variable to hold the highest cosine similarity repr_word w.r.t the current ngram\n",
    "        highest = {'repr_word': '', 'value':0}\n",
    "\n",
    "        # Making sure the Post_ID is correct with the current ngram\n",
    "        # Check if the post_row is empty\n",
    "        if post_row['Post_ID'] == '':\n",
    "            # Assign post_row with the current post_id\n",
    "            post_row['Post_ID'] = post_id\n",
    "        # Else if the post_row is not empty and the Post_ID is different from the current post_id,\n",
    "        elif post_row['Post_ID'] != post_id:\n",
    "            # Append the existing post_row to detected_variables_per_post\n",
    "            detected_variables_per_post.append(post_row)\n",
    "            # Reset post_row for the new post\n",
    "            post_row = {'Post_ID': '', **{word:0 for word in repr_words}}\n",
    "\n",
    "        # Iterate through each representative word and its embedding (converted to numpy)\n",
    "        for repr_word_emb, repr_word in zip(repr_words_emb_df.values, repr_words):\n",
    "            # Calculate cosine similarity\n",
    "            cos_sim = util.cos_sim(ngram_emb, repr_word_emb).item()\n",
    "            # Check if the cosine similarity is above the threshold and above the current highest cosine similarity for the ngram\n",
    "            if (cos_sim >= threshold) and (cos_sim > highest['value']):\n",
    "                # If so, update the highest variable\n",
    "                highest['repr_word'] = repr_word\n",
    "                highest['value'] = cos_sim\n",
    "\n",
    "        # Update the ngram_row and post_row if a representative word was found\n",
    "        if highest['repr_word'] != '':\n",
    "            ngram_row[highest['repr_word']] = 1\n",
    "            post_row[highest['repr_word']] = 1\n",
    "\n",
    "        # Append the ngram_row to detected_variables_per_ngram\n",
    "        detected_variables_per_ngram.append(ngram_row)\n",
    "    # Append the the last post_row to detected_variables_per_post\n",
    "    detected_variables_per_post.append(post_row)\n",
    "\n",
    "    # Create DataFrames for the detected variables\n",
    "    variables_per_post = pd.DataFrame(detected_variables_per_post.copy())\n",
    "    variables_per_ngram = pd.DataFrame(detected_variables_per_ngram.copy())\n",
    "\n",
    "    if save:\n",
    "        # Save detected variables to CSV\n",
    "        variables_per_post.to_csv(per_post_output_csv, index=False)\n",
    "        variables_per_ngram.to_csv(per_ngram_output_csv, index=True)\n",
    "\n",
    "    return variables_per_post, variables_per_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f6e81",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396992ba",
   "metadata": {},
   "source": [
    "## Prepare Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b16ebcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post_ID</th>\n",
       "      <th>clause</th>\n",
       "      <th>n_gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>EXISTING WITH ANXIETY.</td>\n",
       "      <td>EXISTING WITH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>EXISTING WITH ANXIETY.</td>\n",
       "      <td>EXISTING WITH ANXIETY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>EXISTING WITH ANXIETY.</td>\n",
       "      <td>WITH ANXIETY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>My name is Dennis</td>\n",
       "      <td>My name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>My name is Dennis</td>\n",
       "      <td>My name is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>and I don't know if I can handle the stress.</td>\n",
       "      <td>the stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>Thanks for listening xx</td>\n",
       "      <td>Thanks for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>Thanks for listening xx</td>\n",
       "      <td>Thanks for listening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>Thanks for listening xx</td>\n",
       "      <td>for listening xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>Thanks for listening xx</td>\n",
       "      <td>listening xx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2036 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Post_ID                                        clause  \\\n",
       "0     Anxi-1412                        EXISTING WITH ANXIETY.   \n",
       "1     Anxi-1412                        EXISTING WITH ANXIETY.   \n",
       "2     Anxi-1412                        EXISTING WITH ANXIETY.   \n",
       "3     Anxi-1412                             My name is Dennis   \n",
       "4     Anxi-1412                             My name is Dennis   \n",
       "...         ...                                           ...   \n",
       "2031  Anxi-6474  and I don't know if I can handle the stress.   \n",
       "2032  Anxi-6474                       Thanks for listening xx   \n",
       "2033  Anxi-6474                       Thanks for listening xx   \n",
       "2034  Anxi-6474                       Thanks for listening xx   \n",
       "2035  Anxi-6474                       Thanks for listening xx   \n",
       "\n",
       "                     n_gram  \n",
       "0             EXISTING WITH  \n",
       "1     EXISTING WITH ANXIETY  \n",
       "2              WITH ANXIETY  \n",
       "3                   My name  \n",
       "4                My name is  \n",
       "...                     ...  \n",
       "2031             the stress  \n",
       "2032             Thanks for  \n",
       "2033   Thanks for listening  \n",
       "2034       for listening xx  \n",
       "2035           listening xx  \n",
       "\n",
       "[2036 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data()\n",
    "sampled_data = sample_data(data, n=10)\n",
    "cleaned_sampled_data = clean_data(sampled_data)\n",
    "data_meta_df, data_emb_df = embed_data(\n",
    "    cleaned_sampled_data,\n",
    "    load=True,\n",
    "    save=False\n",
    ")\n",
    "data_meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56704236",
   "metadata": {},
   "source": [
    "## Prepare Representative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e55f3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repr_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trouble remembering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flashback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>numb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hopeless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>cyclone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>land degradation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>night shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>job responsibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>debt problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               repr_word\n",
       "0    trouble remembering\n",
       "1                  bored\n",
       "2              flashback\n",
       "3                   numb\n",
       "4               hopeless\n",
       "..                   ...\n",
       "134              cyclone\n",
       "135     land degradation\n",
       "136          night shift\n",
       "137   job responsibility\n",
       "138         debt problem\n",
       "\n",
       "[139 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symptom_repr_words_list = load_symptom_repr_words()\n",
    "risk_factor_repr_words_list = load_risk_factor_repr_words()\n",
    "repr_words_meta_df, repr_words_emb_df = embed_rep_words(\n",
    "    symptoms_rw_list=symptom_repr_words_list,\n",
    "    risk_factors_rw_list=risk_factor_repr_words_list,\n",
    "    load=True\n",
    ")\n",
    "repr_words_meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e5bfb",
   "metadata": {},
   "source": [
    "## Detect Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a13d014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post_ID</th>\n",
       "      <th>clause</th>\n",
       "      <th>n_gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>EXISTING WITH ANXIETY.</td>\n",
       "      <td>EXISTING WITH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>EXISTING WITH ANXIETY.</td>\n",
       "      <td>EXISTING WITH ANXIETY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>EXISTING WITH ANXIETY.</td>\n",
       "      <td>WITH ANXIETY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>My name is Dennis</td>\n",
       "      <td>My name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anxi-1412</td>\n",
       "      <td>My name is Dennis</td>\n",
       "      <td>My name is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>and I don't know if I can handle the stress.</td>\n",
       "      <td>the stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>Thanks for listening xx</td>\n",
       "      <td>Thanks for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>Thanks for listening xx</td>\n",
       "      <td>Thanks for listening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>Thanks for listening xx</td>\n",
       "      <td>for listening xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>Anxi-6474</td>\n",
       "      <td>Thanks for listening xx</td>\n",
       "      <td>listening xx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2036 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Post_ID                                        clause  \\\n",
       "0     Anxi-1412                        EXISTING WITH ANXIETY.   \n",
       "1     Anxi-1412                        EXISTING WITH ANXIETY.   \n",
       "2     Anxi-1412                        EXISTING WITH ANXIETY.   \n",
       "3     Anxi-1412                             My name is Dennis   \n",
       "4     Anxi-1412                             My name is Dennis   \n",
       "...         ...                                           ...   \n",
       "2031  Anxi-6474  and I don't know if I can handle the stress.   \n",
       "2032  Anxi-6474                       Thanks for listening xx   \n",
       "2033  Anxi-6474                       Thanks for listening xx   \n",
       "2034  Anxi-6474                       Thanks for listening xx   \n",
       "2035  Anxi-6474                       Thanks for listening xx   \n",
       "\n",
       "                     n_gram  \n",
       "0             EXISTING WITH  \n",
       "1     EXISTING WITH ANXIETY  \n",
       "2              WITH ANXIETY  \n",
       "3                   My name  \n",
       "4                My name is  \n",
       "...                     ...  \n",
       "2031             the stress  \n",
       "2032             Thanks for  \n",
       "2033   Thanks for listening  \n",
       "2034       for listening xx  \n",
       "2035           listening xx  \n",
       "\n",
       "[2036 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbb1939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_variables, ngram_variables = detect_variables(\n",
    "    data_meta_df=data_meta_df,\n",
    "    data_emb_df=data_emb_df,\n",
    "    repr_words_meta_df=repr_words_meta_df,\n",
    "    repr_words_emb_df=repr_words_emb_df,\n",
    "    threshold=0.7,\n",
    "    load=True,\n",
    "    save=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18466607",
   "metadata": {},
   "source": [
    "## Sampling Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77c1e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_num = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "108479d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>can't stop worrying</th>\n",
       "      <th>lack motivation</th>\n",
       "      <th>exhausted</th>\n",
       "      <th>guilt</th>\n",
       "      <th>afraid</th>\n",
       "      <th>failure</th>\n",
       "      <th>sleeping too much</th>\n",
       "      <th>heatwave</th>\n",
       "      <th>Post_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Depr-5053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   can't stop worrying  lack motivation  exhausted  guilt  afraid  failure  \\\n",
       "6                    1                1          1      1       1        1   \n",
       "\n",
       "   sleeping too much  heatwave    Post_ID  \n",
       "6                  1         1  Depr-5053  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = post_variables.iloc[post_num-1:post_num].columns[(post_variables.iloc[post_num-1:post_num] == 1).any()]\n",
    "new_cols = cols.append(pd.Index(['Post_ID']))\n",
    "post_id = post_variables.iloc[post_num-1:post_num]['Post_ID'].values[0]\n",
    "post_variables.iloc[post_num-1:post_num][new_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fcd835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost motivation and direction\n",
      "I feel guilty because I am wasting time and not doing anything useful. I feel like I have no purpose. I feel guilty because I have no excuse for complaining Other people have much worse situations. I normally am very goal orientated and feel bad if I am not achieving things. I am marking time waiting for summer to finish. I hate the heat. I am scared in case fires start and I don't like going out in case I get sunburnt. I had a good job which I left in October. I had been struggling because my husband kept getting sick. He spent 6 weeks in hospital Sept-Oct '14 just after I started work and came home so weak he could barely walk from one room to the next. (we previously enjoyed bushwalking) He then had lots of doctors appointments and tests to plan for further surgery to prevent him having the same problem. I was very stressed and always worried about him. I couldn't afford to take off much time for carers leave because I hadn't worked long enough. My plan was to arrange a holiday and see how I felt after having a break. Unfortunately one holiday was cancelled because of the initial illness, the second holiday (which was timed to coincide with a public holiday and flexing my hours to limit the days off) I came home exhausted as my husband became sick the night we left and I got very little sleep as I had to look after him in the night. After his surgery in July we booked a 2 week holiday for September. The surgery failed and was repeated 5 times. After the last failure we decided not to contact the hospital or doctor as we couldn't cope with another failed attempt. We will just manage as best as we can. 2 weeks before the holiday I broke my ankle so the holiday was cancelled. At that point I decided to resign rather than see how I felt after a break.\n",
      "02-01-2016\n"
     ]
    }
   ],
   "source": [
    "print(sampled_data[sampled_data[\"Post_ID\"] == post_id]['Post_Title'].item())\n",
    "print(sampled_data[sampled_data[\"Post_ID\"] == post_id]['Post_Content'].item())\n",
    "print(sampled_data[sampled_data[\"Post_ID\"] == post_id]['Post_Date'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca7f60fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>can't stop worrying</th>\n",
       "      <th>lack motivation</th>\n",
       "      <th>exhausted</th>\n",
       "      <th>guilt</th>\n",
       "      <th>afraid</th>\n",
       "      <th>failure</th>\n",
       "      <th>sleeping too much</th>\n",
       "      <th>heatwave</th>\n",
       "      <th>clause</th>\n",
       "      <th>n_gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [can't stop worrying, lack motivation, exhausted, guilt, afraid, failure, sleeping too much, heatwave, clause, n_gram]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_col = cols.append(pd.Index(['clause', 'n_gram']))\n",
    "ngram_variables[(ngram_variables[\"Post_ID\"] == post_id) & (ngram_variables.sum(axis=1, numeric_only=True) == 1)][detail_col]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
